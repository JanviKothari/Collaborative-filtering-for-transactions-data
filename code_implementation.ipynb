{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "A collaborative filtering model can be built once given a user-item matrix with ratings.  \n",
    "\n",
    "### Question:\n",
    "* Build a Model that recommends to the user the \"first item\" they may want to place into their \"basket\"\n",
    "  * Input: user - customer ID\n",
    "  * Returns: ranked list of items (product IDs), that the user is most likely to want to put in his/her (empty) \"basket\"\n",
    "\n",
    "In `recommend_1.csv`, we provide a list of customer IDs. If you select option 1, use this data to generate a csv file that indicates top 10 recommendations for each of the customers. Note the order of the recommended products should be ordered by user preference, with the most preferred item in the beginning.\n",
    "\n",
    "Sample output:\n",
    "\n",
    "`customerId, recommendedProducts\n",
    "1,0|1|2|3|4|5|6|7|8|9\n",
    "2,8|3|1|2|4|7|9|10|11|13\n",
    "3,20|21|22|23|24|25|26|27|28|29\n",
    "...\n",
    "`\n",
    "\n",
    "\n",
    "## Notes on the business use case for evaluation\n",
    "* The goal of this modeling project is to recommend to the user a list of items that they are most likely to purchase (option 1) or add to their existing basket (option 2).  \n",
    "* As you are selecting metrics, please keep in mind that \n",
    " 1. the primary goal is to successfully recommend as many items in your list that they may be inclined to purchase/add, and \n",
    " 2. the secondary goal is that the items are ordered by the user's inclination (the more inclined they are, the higher up in your list of 10 recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import chain\n",
    "from ordered_set import OrderedSet\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_colwidth',-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I. Reading the transactions data to train the algorithm from and reading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerId</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2|2|23|68|68|111|29|86|107|152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>111|107|29|11|11|11|33|23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>164|227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2|2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerId                        products\n",
       "0  0           20                            \n",
       "1  1           2|2|23|68|68|111|29|86|107|152\n",
       "2  2           111|107|29|11|11|11|33|23     \n",
       "3  3           164|227                       \n",
       "4  5           2|2                           "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trx_df = pd.read_csv('../data/trx_data.csv')\n",
    "trx_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerId\n",
       "0  1553      \n",
       "1  20400     \n",
       "2  19750     \n",
       "3  6334      \n",
       "4  27773     "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('../data/recommend_1.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing all the customer IDs from the test data to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_ID_test = test_df['customerId'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a named tuple to format our recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECOMMENDATIONS = namedtuple(\"RECOMMENDATIONS\", [\"customerId\", \"products\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II. Preprocessing the data\n",
    "\n",
    "1. The data here is transactional data containing products purchased for each user for each transaction. \n",
    "2. These are separated using a delimiter. For this, I am creating a function which returns the list of products purchased for that transaction, removing the delimiter\n",
    "3. - Now, in order to implement collaborative filtering, it is essential that our data be in terms of a user-item interactions matrix, containing ratings by each user for the item they have purchased.\n",
    "  - However, the transactional data that we have here does not have any such \"explicit\" feedback. Therefore, I am substituting the counts of each item purchased by that user throughout all the transactions as our ratings. My intuition behind using frequency of item purchased is such that if an item is purchased frequently by a customer, it is a \"popular\" item. Thus that should be factored in while creating our utility matrix of customer-to-product interactions.\n",
    "  - One very important thing to note here is that this causes the *loss of temporal information in the data*, since I am aggregating all the sequential transactions into a single list of items purchased.\n",
    "4. In order to process the data in the above mentioned format, I am first creating a simple dataframe which shows for each customerId, the productId and frequency of purchase of that product by that customer. Given that there could be lots of products, we will have no count values for those items that a customer has not purchased. Thus, I am replacing the NaN values with 0 to display the frequency as 0.\n",
    "5. Another approach is to instead simply substitute the ratings with a binary value, which could be represented as:\n",
    "    - 1: if customer has purchased an item based on the transactions data\n",
    "    - 0: if customer has not purchased an item based on the transactions data\n",
    "    \n",
    "   However, we lose out on the popularity factor of the items purchased. Though we will not be using this approach, I have created a function below which returns the user-item matrix in terms of binary values for future use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_trx_on_symbol(transactions_df, symbol):\n",
    "    '''Removing the pipe symbol from transactions and storing as list.\n",
    "    Args:\n",
    "        transactions_df (pd.DataFrame): Customer purchase history.\n",
    "        symbol (str): Delimiter for splitting product IDs.\n",
    "    Returns:\n",
    "        processed_df (pd.DataFrame): Processed dataframe consisting of \n",
    "            customer purchase history with products in a list.\n",
    "    '''\n",
    "    \n",
    "    processed_df = transactions_df.copy()\n",
    "    processed_df['products'] = processed_df['products'].apply(lambda x: [int(each) for each in x.split(symbol)])\n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def get_product_counts(processed_df):\n",
    "    '''Returns user-item interaction matrix by computing frequency of each product purchased by user.\n",
    "    Args:\n",
    "        processed_df (pd.DataFrame): Customer purchase history after splitting on delimiter.\n",
    "    Returns:\n",
    "        frequency_df (pd.DataFrame): Processed dataframe consisting of user-item-frequency.\n",
    "    '''\n",
    "    \n",
    "    user_item_int = processed_df.set_index('customerId')['products'].apply(pd.Series).reset_index()\n",
    "    user_items =  user_item_int.melt(id_vars=['customerId'], value_name=\"products\").\\\n",
    "                    sort_values(by=['customerId']).dropna().drop(\"variable\",axis=1).\\\n",
    "                    reset_index(drop=True)\n",
    "    user_items = user_items.astype(int)\n",
    "    frequency_df = user_items.groupby(['customerId','products']).size().reset_index(name='purchaseFrequency')\n",
    "    \n",
    "    return frequency_df\n",
    "\n",
    "\n",
    "def get_product_purchased(processed_df):\n",
    "    '''Returns user-item interaction matrix consisting of binary value based user's purchase history\n",
    "    Args:\n",
    "        processed_df (pd.DataFrame): Customer purchase history after splitting on delimiter.\n",
    "    Returns:\n",
    "        purchased_df (pd.DataFrame): Processed dataframe consisting of user-item-binary.\n",
    "    '''\n",
    "    \n",
    "    user_item_int = processed_df.set_index('customerId')['products'].apply(pd.Series).reset_index()\n",
    "    purchased_df =  user_item_int.melt(id_vars=['customerId'], value_name=\"products\").\\\n",
    "                    sort_values(by=['customerId']).dropna().drop(\"variable\",axis=1).\\\n",
    "                    reset_index(drop=True)\n",
    "    purchased_df['purchasedOrNot'] = 1\n",
    "    purchased_df.drop_duplicates(keep='first',inplace=True)\n",
    "    purchased_df = purchased_df.astype(int)\n",
    "    purchased_df.reset_index(drop=True,inplace=True)\n",
    "    \n",
    "    return purchased_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data using above functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerId</th>\n",
       "      <th>products</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[2, 2, 23, 68, 68, 111, 29, 86, 107, 152]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[111, 107, 29, 11, 11, 11, 33, 23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[164, 227]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[2, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerId                                   products\n",
       "0  0           [20]                                     \n",
       "1  1           [2, 2, 23, 68, 68, 111, 29, 86, 107, 152]\n",
       "2  2           [111, 107, 29, 11, 11, 11, 33, 23]       \n",
       "3  3           [164, 227]                               \n",
       "4  5           [2, 2]                                   "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df = split_trx_on_symbol(trx_df,'|')\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerId</th>\n",
       "      <th>products</th>\n",
       "      <th>purchaseFrequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerId  products  purchaseFrequency\n",
       "0  0           1         2                \n",
       "1  0           13        1                \n",
       "2  0           19        3                \n",
       "3  0           20        1                \n",
       "4  0           31        2                "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchase_frequency_df = get_product_counts(processed_df)\n",
    "purchase_frequency_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, I am widening the purchase_frequency_df from above by using pivot, where the columns are all product IDs and a record represents one customer in the dataframe.\n",
    "\n",
    "Note: I am using the raw frequencies and not demeaning them across items or users since I want the interactions to be in their original representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerId</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerId    0    1    2    3    4    5    6    7    8  ...  290  291  \\\n",
       "0  0           0.0  2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "1  1           0.0  0.0  6.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "2  2           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "3  3           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "4  4           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "   292  293  294  295  296  297  298  299  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  1.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df = purchase_frequency_df.pivot(index='customerId', columns='products', values='purchaseFrequency').fillna(0)\n",
    "interactions_df.reset_index(drop=False, inplace=True)\n",
    "interactions_df = interactions_df.rename_axis(None, axis=1)\n",
    "interactions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24429, 301)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### III. Evaluating our models\n",
    "In order to evaluate which model to whose, we divide the interactions data into a training and a validation set with the ratio of 90:10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((21986, 301), (2443, 301))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val = train_test_split(interactions_df, test_size=0.10, random_state=42)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing all the customer IDs from the validation set to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_ID_val = X_val[\"customerId\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for evaluation\n",
    "*Recall@k*: Recall is a measure which gives us the proportion of relevant items found in the top-k recommendations. Here I am interpreting relevant items as the items which are in a customer's purchase history. The highest recall suggests that the recommendations captures the maximum relevant products.\n",
    "*RECALL@k* = (Number of recommended items @k that are relevant) / (total number of relevant items)\n",
    "\n",
    "Finally, the final recall@k for each model will be averaged across all users in the val/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_at_k(recommendations_df,preprocessed_original_df,k):\n",
    "    '''Computing average recall over all the customers recommendations for different k values.\n",
    "    Args:\n",
    "        recommendations_df (pd.DataFrame): List of recommended products for each customer in test data.\n",
    "        preprocessed_original_df (pd.DataFrame): Customer purchase history dataframe\n",
    "        k (int): The position in the top-k recommendations where we want to obtain the highest recall.\n",
    "    Returns:\n",
    "        averageRecall (float): Average recall value\n",
    "    '''\n",
    "    \n",
    "    recommendations_string_df = split_trx_on_symbol(recommendations_df,'|')\n",
    "    sumOfRecall = 0\n",
    "    for index, row in recommendations_string_df.iterrows():\n",
    "        customerId = row['customerId']\n",
    "        recommendedProducts = row['products']\n",
    "        relevantProducts = preprocessed_original_df[preprocessed_original_df['customerId'] == customerId]\\\n",
    "            ['products'].tolist()\n",
    "        numberOfRelevantProducts = len(relevantProducts)\n",
    "        if k == 1:\n",
    "            numberOfRelevantRecommendedProducts = len(list(set(recommendedProducts[:1]) & set(relevantProducts)))\n",
    "            recall = numberOfRelevantRecommendedProducts/numberOfRelevantProducts\n",
    "            sumOfRecall += recall\n",
    "        elif k == 5:\n",
    "            numberOfRelevantRecommendedProducts = len(list(set(recommendedProducts[:5]) & set(relevantProducts)))\n",
    "            recall = numberOfRelevantRecommendedProducts/numberOfRelevantProducts\n",
    "            sumOfRecall += recall\n",
    "        elif k == 10:\n",
    "            numberOfRelevantRecommendedProducts = len(list(set(recommendedProducts) & set(relevantProducts)))\n",
    "            recall = numberOfRelevantRecommendedProducts/numberOfRelevantProducts\n",
    "            sumOfRecall += recall\n",
    "        \n",
    "    averageRecall = sumOfRecall/recommendations_string_df.shape[0]\n",
    "    \n",
    "    return averageRecall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IV. Implementing model for predicting 10 product recommendations to each user.\n",
    "\n",
    "1. I am building a model for the question: \n",
    "    - Option 1 Model: In recommend_1.csv, we provide a list of customer IDs. If you select option 1, use this data to generate a csv file that indicates top 10 recommendations for each of the customers. Note the order of the recommended products should be ordered by user preference, with the most preferred item in the beginning.\n",
    "2. There are different approaches to building a collaborative filtering model. \n",
    "    - First approach: *Model-based CF* using matrix-factorization: *Matrix factorization* is an approach which works by factorizing the customer-product interactions matrix to find latent features in the data. We obtain multiple matrices which are user-features matrix, item-features matrix, and a matrix having weights. On taking the product of these matrices, we get an approximation of the original interactions matrix, thus helping in our recommendations.\n",
    "    - Second approach: *Memory-based CF*: I am looking at an approach which incorporates customer's purchase history to find most similar items. The similarity among items is calculated by finding the *nearest neighbors* of an item based on a *distance measure* which could be cosine, euclidean, etc. Thus the nearest 10 neighbors can be our recommendations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 1: Matrix factorisation using Singular Value Decomposition and then making recommendations.\n",
    "The function *generate_svd_results()* generates the approximated matrix after applying svd, which is then passed as an argument to the *make_recommendations_svd()* which returns the dataframe containing recommendations for all customers in our val/test set.\n",
    "\n",
    "In this approach, for each customer in the validation/test data, I am returning 10 items which have the highest scores in the approximated matrix from SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_svd_results(interactions_df,numberOfLatentFeatures):\n",
    "    '''Computing matrix factorization using SVD and returning the approximated matrix as a dataframe.\n",
    "    Args:\n",
    "        interactions_df (pd.DataFrame): Customer purchase history in the form of user-item interactions.\n",
    "        numberOfLatentFeatures (int): Number of latent features we want the interactions to be broken down into.\n",
    "    Returns:\n",
    "        preds_df (pd.DataFrame): Approximated matrix after applying SVD.\n",
    "    '''\n",
    "    \n",
    "    frequency_interactions = interactions_df.values\n",
    "    U, sigma, Vt = svds(frequency_interactions, k = numberOfLatentFeatures)\n",
    "    sigma = np.diag(sigma)\n",
    "    predicted_scores = np.dot(np.dot(U, sigma), Vt)\n",
    "    preds_df = pd.DataFrame(predicted_scores, columns = interactions_df.columns)\n",
    "    return preds_df\n",
    "\n",
    "\n",
    "def make_recommendations_svd(interactions_df, predictions_df, test_customers):\n",
    "    '''Generating the recommendations dataframe using the approximated matrix obtained from SVD.\n",
    "    Args:\n",
    "        interactions_df (pd.DataFrame): Customer purchase history in the form of user-item interactions.\n",
    "        predictions_df (pd.DataFrame): Approximated matrix after applying SVD.\n",
    "        test_customers (list): List of customers for whom we want to make the recommendations.\n",
    "    Returns:\n",
    "        recommendations_df (pd.DataFrame): Final recommendations for our validation/test data.\n",
    "    '''\n",
    "    \n",
    "    final_recommendations = []\n",
    "    for customerId in test_customers:\n",
    "        user_index = interactions_df[interactions_df.customerId == customerId].index[0]\n",
    "        sorted_user_predictions = pd.DataFrame(predictions_df.iloc[user_index].sort_values(ascending=False)).reset_index()\n",
    "        recommended_items = sorted_user_predictions[\"index\"][:10]\n",
    "        recommended_items = map(str,recommended_items)\n",
    "        recommended_string = \"|\".join(recommended_items)\n",
    "        rec = RECOMMENDATIONS(customerId, recommended_string)\n",
    "        final_recommendations.append(rec)\n",
    "    recommendations_df = pd.DataFrame.from_records(\n",
    "       final_recommendations,\n",
    "       columns=RECOMMENDATIONS._fields\n",
    "    )\n",
    "\n",
    "    return recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running SVD for different number of latent features from [25,50,75,100,125,150]and generating recall at 1st, 5th & 10th positions to determine which model works best for the customers in the validation set. \n",
    "\n",
    "i. numberOfLatentFeatures = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1:  0.1733239721476204\n",
      "Recall@5:  0.336201845814706\n",
      "Recall@10:  0.4033837278236912\n"
     ]
    }
   ],
   "source": [
    "predictions_df = generate_svd_results(interactions_df,25)\n",
    "predictions_df.drop('customerId', axis=1, inplace=True)\n",
    "recommendations_df = make_recommendations_svd(interactions_df, predictions_df, customer_ID_val)\n",
    "\n",
    "recall_at_1 = compute_recall_at_k(recommendations_df,purchase_frequency_df,1)\n",
    "print(\"Recall@1: \",recall_at_1)\n",
    "recall_at_5 = compute_recall_at_k(recommendations_df,purchase_frequency_df,5)\n",
    "print(\"Recall@5: \",recall_at_5)\n",
    "recall_at_10 = compute_recall_at_k(recommendations_df,purchase_frequency_df,10)\n",
    "print(\"Recall@10: \",recall_at_10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. numberOfLatentFeatures = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1:  0.23348804000346934\n",
      "Recall@5:  0.46374240397752176\n",
      "Recall@10:  0.5557809712028159\n"
     ]
    }
   ],
   "source": [
    "predictions_df = generate_svd_results(interactions_df,50)\n",
    "predictions_df.drop('customerId', axis=1, inplace=True)\n",
    "recommendations_df = make_recommendations_svd(interactions_df, predictions_df, customer_ID_val)\n",
    "\n",
    "recall_at_1 = compute_recall_at_k(recommendations_df,purchase_frequency_df,1)\n",
    "print(\"Recall@1: \",recall_at_1)\n",
    "recall_at_5 = compute_recall_at_k(recommendations_df,purchase_frequency_df,5)\n",
    "print(\"Recall@5: \",recall_at_5)\n",
    "recall_at_10 = compute_recall_at_k(recommendations_df,purchase_frequency_df,10)\n",
    "print(\"Recall@10: \",recall_at_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. numberOfLatentFeatures = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1:  0.28024834523696385\n",
      "Recall@5:  0.5703779834385959\n",
      "Recall@10:  0.6621810123452504\n"
     ]
    }
   ],
   "source": [
    "predictions_df = generate_svd_results(interactions_df,75)\n",
    "predictions_df.drop('customerId', axis=1, inplace=True)\n",
    "recommendations_df = make_recommendations_svd(interactions_df, predictions_df, customer_ID_val)\n",
    "\n",
    "recall_at_1 = compute_recall_at_k(recommendations_df,purchase_frequency_df,1)\n",
    "print(\"Recall@1: \",recall_at_1)\n",
    "recall_at_5 = compute_recall_at_k(recommendations_df,purchase_frequency_df,5)\n",
    "print(\"Recall@5: \",recall_at_5)\n",
    "recall_at_10 = compute_recall_at_k(recommendations_df,purchase_frequency_df,10)\n",
    "print(\"Recall@10: \",recall_at_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv. numberOfLatentFeatures = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1:  0.3038904381772921\n",
      "Recall@5:  0.6436990277847527\n",
      "Recall@10:  0.7412760732203166\n"
     ]
    }
   ],
   "source": [
    "predictions_df = generate_svd_results(interactions_df,100)\n",
    "predictions_df.drop('customerId', axis=1, inplace=True)\n",
    "recommendations_df = make_recommendations_svd(interactions_df, predictions_df, customer_ID_val)\n",
    "\n",
    "recall_at_1 = compute_recall_at_k(recommendations_df,purchase_frequency_df,1)\n",
    "print(\"Recall@1: \",recall_at_1)\n",
    "recall_at_5 = compute_recall_at_k(recommendations_df,purchase_frequency_df,5)\n",
    "print(\"Recall@5: \",recall_at_5)\n",
    "recall_at_10 = compute_recall_at_k(recommendations_df,purchase_frequency_df,10)\n",
    "print(\"Recall@10: \",recall_at_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v. numberOfLatentFeatures = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1:  0.31090026216419364\n",
      "Recall@5:  0.6536384313418756\n",
      "Recall@10:  0.7492118822058108\n"
     ]
    }
   ],
   "source": [
    "predictions_df = generate_svd_results(interactions_df,125)\n",
    "predictions_df.drop('customerId', axis=1, inplace=True)\n",
    "recommendations_df = make_recommendations_svd(interactions_df, predictions_df, customer_ID_val)\n",
    "\n",
    "recall_at_1 = compute_recall_at_k(recommendations_df,purchase_frequency_df,1)\n",
    "print(\"Recall@1: \",recall_at_1)\n",
    "recall_at_5 = compute_recall_at_k(recommendations_df,purchase_frequency_df,5)\n",
    "print(\"Recall@5: \",recall_at_5)\n",
    "recall_at_10 = compute_recall_at_k(recommendations_df,purchase_frequency_df,10)\n",
    "print(\"Recall@10: \",recall_at_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vi. numberOfLatentFeatures = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1:  0.31090026216419364\n",
      "Recall@5:  0.6536384313418756\n",
      "Recall@10:  0.7492118822058108\n"
     ]
    }
   ],
   "source": [
    "predictions_df = generate_svd_results(interactions_df,150)\n",
    "predictions_df.drop('customerId', axis=1, inplace=True)\n",
    "recommendations_df = make_recommendations_svd(interactions_df, predictions_df, customer_ID_val)\n",
    "\n",
    "recall_at_1 = compute_recall_at_k(recommendations_df,purchase_frequency_df,1)\n",
    "print(\"Recall@1: \",recall_at_1)\n",
    "recall_at_5 = compute_recall_at_k(recommendations_df,purchase_frequency_df,5)\n",
    "print(\"Recall@5: \",recall_at_5)\n",
    "recall_at_10 = compute_recall_at_k(recommendations_df,purchase_frequency_df,10)\n",
    "print(\"Recall@10: \",recall_at_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding:** Looking at the results of *SVD* by inputting different values of *hidden features* in the algorithm, we see that after a certain number i.e. 125, the recall does not improve. Thus, the highest recall@10 obtained by SVD is ~75% i.e. out of all 10 recommended items, 75% of the items are relevant. On the other hand, the recall@1 which is the result of the first recommended item to be relevant is 31%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2: Recommending nearest neighbors using similarity and frequency of items purchased\n",
    "This approach is described using the following steps:\n",
    "\n",
    " 1) For each customer in our test data, first I obtain the 10 nearest neighbors of each item purchased by that customer. The nearest neighbors are calculated using the Minkowski distance among all interactions. Thus I have neighbors as well as their distances.\n",
    "    - For instance, if a customer's purchase history contains 3 items. Now, we have 30 neighbors & their distances (10 for each of the purchase item).\n",
    "\n",
    " 2) Now, I compute the weighted distances (score) of each of these items' neighbors by multiplying the inverse of distances with that item's frequency. Thus, we get the \"nearer\" items to have higher scores and the \"farther\" items to have lowest scores\n",
    " \n",
    " 3) Then, I obtain the 10 unique items having the highest weighted distances. These are our final recommendations.  \n",
    "    - In the case explained above we obtain the 10 unique recommendations based 30 weighted distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recommendations_neighbors(interactions_df,purchase_frequency_df,test_customers,metric,algorithm):\n",
    "    '''\n",
    "    Args:\n",
    "        interactions_df (pd.DataFrame): Customer purchase history in the form of user-item interactions.\n",
    "        purchase_frequency_df (pd.DataFrame): Customer purchase history in the preprocessed format\n",
    "            of user-item-frequency\n",
    "        test_customers (list): List of customers for whom we want to make the recommendations.\n",
    "    Returns:\n",
    "        recommendations_df (pd.DataFrame): Final recommendations for our validation/test data.\n",
    "    '''\n",
    "    \n",
    "    final_recommendations = []\n",
    "    frequency_interactions = interactions_df.values\n",
    "    model_knn = NearestNeighbors(algorithm=algorithm, metric=metric)\n",
    "    model_knn.fit(frequency_interactions)\n",
    "    \n",
    "    for customerId in test_customers:\n",
    "        all_neighbors = []\n",
    "        all_distances = []\n",
    "        relevantRecords = purchase_frequency_df[purchase_frequency_df['customerId'] == customerId]\n",
    "        relevantProducts = relevantRecords['products'].tolist()\n",
    "        \n",
    "        for index, row in relevantRecords.iterrows():\n",
    "            weight = row['purchaseFrequency']\n",
    "            distances,neighbors = model_knn.kneighbors(interactions_df.iloc[row['products']].values.reshape(1,-1), n_neighbors=11)\n",
    "            weightedDistances = weight * (1/(distances + 1e-5))\n",
    "            all_neighbors.append(list(neighbors[0])[:])\n",
    "            all_distances.append(list(weightedDistances[0][:]))\n",
    "            \n",
    "        all_neighbors_flat = list(chain.from_iterable(all_neighbors))\n",
    "        all_distances_flat = list(chain.from_iterable(all_distances))\n",
    "        \n",
    "        products_scores_df = pd.DataFrame({'products':all_neighbors_flat,'scores':all_distances_flat})\n",
    "        products_scores_df.sort_values('scores',ascending=False,inplace=True)\n",
    "        \n",
    "        recommended_items = list(OrderedSet(products_scores_df['products']))[:10]\n",
    "        recommended_items = map(str,recommended_items)\n",
    "        recommended_string = \"|\".join(recommended_items)\n",
    "        rec = RECOMMENDATIONS(customerId,recommended_string)\n",
    "        final_recommendations.append(rec)\n",
    "        \n",
    "    recommendations_df = pd.DataFrame.from_records(\n",
    "        final_recommendations,\n",
    "        columns=RECOMMENDATIONS._fields)\n",
    "    return recommendations_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running neearest neigbors for default number of neighbors from and generating recall at 1st, 5th & 10th positions to determine which model works best for the customers in the validation set. \n",
    "\n",
    "Here, I am modifying the interactions_df to pivot such the shape is number of items x number of customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>28581</th>\n",
       "      <th>28583</th>\n",
       "      <th>28585</th>\n",
       "      <th>28588</th>\n",
       "      <th>28590</th>\n",
       "      <th>28593</th>\n",
       "      <th>28596</th>\n",
       "      <th>28598</th>\n",
       "      <th>28604</th>\n",
       "      <th>28605</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24429 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3    4    5    6    7    8    9  ...  28581  28583  28585  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  3.0  ...  0.0    0.0    0.0     \n",
       "1  2.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0  ...  0.0    0.0    0.0     \n",
       "2  0.0  6.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0  6.0  ...  0.0    0.0    0.0     \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0    0.0    0.0     \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0    0.0    0.0     \n",
       "\n",
       "   28588  28590  28593  28596  28598  28604  28605  \n",
       "0  0.0    0.0    0.0    0.0    0.0    0.0    0.0    \n",
       "1  0.0    0.0    0.0    0.0    0.0    0.0    0.0    \n",
       "2  0.0    0.0    0.0    0.0    0.0    0.0    0.0    \n",
       "3  0.0    0.0    0.0    0.0    0.0    0.0    0.0    \n",
       "4  0.0    0.0    0.0    0.0    0.0    0.0    0.0    \n",
       "\n",
       "[5 rows x 24429 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions_df_nn = purchase_frequency_df.pivot(index='products', columns='customerId', values='purchaseFrequency').fillna(0)\n",
    "interactions_df_nn.reset_index(drop=True, inplace=True)\n",
    "interactions_df_nn = interactions_df_nn.rename_axis(None, axis=1)\n",
    "interactions_df_nn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall@1:  0.4008043613396813\n",
      "Recall@5:  0.8483730142301252\n",
      "Recall@10:  0.9619563150581717\n"
     ]
    }
   ],
   "source": [
    "## Here, I am using \"minkowski\" distance to calculate similarity among items\n",
    "recommendations_df_nn = make_recommendations_neighbors(interactions_df_nn,purchase_frequency_df,\\\n",
    "                                                                       customer_ID_val,'minkowski','ball_tree')\n",
    "\n",
    "recall_at_1 = compute_recall_at_k(recommendations_df_nn,purchase_frequency_df,1)\n",
    "print(\"Recall@1: \",recall_at_1)\n",
    "recall_at_5 = compute_recall_at_k(recommendations_df_nn,purchase_frequency_df,5)\n",
    "print(\"Recall@5: \",recall_at_5)\n",
    "recall_at_10 = compute_recall_at_k(recommendations_df_nn,purchase_frequency_df,10)\n",
    "print(\"Recall@10: \",recall_at_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding:** Wow! Now, by getting nearest neighbors using a *distance-based \"similarity\" measure*, we get a much higher recall@10 of 96%\n",
    "On the other hand, recall@1 i.e. recall for the first position is 40%, which is higher than what we observed from results of SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining the algorithm: As we have observed, nearest neighbors outperforms matrix factorisation by a large margin. So, if we are interested in obtaining all 10 recommended items to be as relevant as possible, or even if we are interested in the first item, or the first 5 items recommended to be highly relevant, I choose a nearest neighbor approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have chosen our model based on the recall scores, there is one more metric which we should consider:  \n",
    "1. *Discounted Cumulative Gain (DCG@K)*:  DCG is a measure of the top-k recommendations' **ranking quality**. What this suggests is that highly relevant items should be recommended at the top. \n",
    "2. DCG@k = SUM_over_k(relevance_score(i)/log2(i+1)) where i is the rank for which DCG is being calculated' i is in range of 1 to 10 for our 10 recommender system.\n",
    "3. Based on our business problem, it could be important that we would care for a higher value of DCG at the first 5 results, thus DCG could be used as an evaluation metric for selecting the most appropriate model.\n",
    "    - This could be further enhanced by using Normalized DCG (NDCG), when we have the information about the \"true ranking\" of each product across all products in the dataset. Alternatively, we could frequency of items purchased across all users as a true ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_recommendations_df = make_recommendations_neighbors(interactions_df_nn,purchase_frequency_df,customer_ID_test\n",
    "                                                   ,'minkowski','ball_tree')\n",
    "test_recommendations_df.rename(columns = {'products':'recommendedProducts'},inplace=True)\n",
    "test_recommendations_df.to_csv('../output/test_output.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V. Future Work\n",
    "\n",
    "1. Firstly, I would include DCG@k metric for our evaluations, in order to select the best model.\n",
    "2. Second, a modeling approach to try would be using Autoencoders for making recommendations.\n",
    "3. Third, work on ways to include temporal information in our recommender systems.\n",
    "\n",
    "\n",
    "Note: Throughout the notebook, I have tried to keep the code simple, readable and reusable by creating functions specific to my analysis. However, in order to productionize this code, we could store our modeling algorithms into packages and libraries, thus making them callable. We should also ensure that inputs to the commonly used methods are generalized, and outputs from our modeling results are in the same format. The outputs could be passed on to a simple dashboard or JSON formats, for monitoring or evaluation purposes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
